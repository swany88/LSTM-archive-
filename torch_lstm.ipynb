{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Collection\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2516 entries, 2014-11-11 to 2024-11-08\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Open          2516 non-null   float64\n",
      " 1   High          2516 non-null   float64\n",
      " 2   Low           2516 non-null   float64\n",
      " 3   Close         2516 non-null   float64\n",
      " 4   Volume        2516 non-null   int64  \n",
      " 5   Dividends     2516 non-null   float64\n",
      " 6   Stock Splits  2516 non-null   float64\n",
      "dtypes: float64(6), int64(1)\n",
      "memory usage: 157.2 KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NAN values in data after preprocessing:\n",
      " QCOM_Open                      0\n",
      "QCOM_High                      0\n",
      "QCOM_Low                       0\n",
      "QCOM_Close                     0\n",
      "QCOM_Volume                    0\n",
      "QCOM_Dividends                 0\n",
      "QCOM_Stock Splits              0\n",
      "QCOM_SMA_20                    0\n",
      "QCOM_EMA_20                    0\n",
      "QCOM_BB_Middle                 0\n",
      "QCOM_BB_Upper                  0\n",
      "QCOM_BB_Lower                  0\n",
      "QCOM_RSI                       0\n",
      "QCOM_MACD                      0\n",
      "QCOM_MACD_Signal               0\n",
      "QCOM_OBV                       0\n",
      "QCOM_ATR                       0\n",
      "GDP                            0\n",
      "Interest_Rates                 0\n",
      "Consumer_Confidence            0\n",
      "Industrial_Production          0\n",
      "Unemployment_Rate              0\n",
      "Retail_Sales                   0\n",
      "Housing_Starts                 0\n",
      "Corporate_Profits              0\n",
      "Inflation_Rate                 0\n",
      "Economic_Policy_Uncertainty    0\n",
      "SP500                          0\n",
      "VIX                            0\n",
      "Technology_ETF                 0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import hashlib\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from fredapi import Fred\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# %% Define stock symbol, api key, functions for retrieving stock data\n",
    "symbol = 'QCOM'\n",
    "\n",
    "# FRED API key\n",
    "fred_api_key = os.getenv('FRED_API_KEY')\n",
    "fred = Fred(api_key=fred_api_key)\n",
    "\n",
    "def make_tz_naive(df):\n",
    "    if df.index.tzinfo is not None:\n",
    "        df.index = df.index.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def get_stock_data(symbol, start_date, end_date):\n",
    "    return make_tz_naive(yf.Ticker(symbol).history(start=start_date, end=end_date))\n",
    "\n",
    "def calculate_technical_indicators(data):\n",
    "    result = data.copy()\n",
    "    close = result['Close']\n",
    "    result['SMA_20'] = close.rolling(window=20).mean()\n",
    "    result['EMA_20'] = close.ewm(span=20, adjust=False).mean()\n",
    "    result['BB_Middle'] = result['SMA_20']\n",
    "    bb_std = close.rolling(window=20).std()\n",
    "    result['BB_Upper'] = result['BB_Middle'] + 2 * bb_std\n",
    "    result['BB_Lower'] = result['BB_Middle'] - 2 * bb_std\n",
    "    delta = close.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    result['RSI'] = 100 - (100 / (1 + rs))\n",
    "    ema_12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = close.ewm(span=26, adjust=False).mean()\n",
    "    result['MACD'] = ema_12 - ema_26\n",
    "    result['MACD_Signal'] = result['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    result['OBV'] = (np.sign(delta) * result['Volume']).fillna(0).cumsum()\n",
    "\n",
    "    #? New volatility indicator: Average True Range (ATR)\n",
    "    tr1 = result['High'] - result['Low']\n",
    "    tr2 = abs(result['High'] - result['Close'].shift())\n",
    "    tr3 = abs(result['Low'] - result['Close'].shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    result['ATR'] = tr.rolling(window=14).mean()\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_economic_indicators(fred, start_date, end_date):\n",
    "    fred_series = {\n",
    "        'GDP': 'GDP', 'Interest_Rates': 'FEDFUNDS', 'Consumer_Confidence': 'UMCSENT',\n",
    "        'Industrial_Production': 'INDPRO', 'Unemployment_Rate': 'UNRATE',\n",
    "        'Retail_Sales': 'RSAFS', 'Housing_Starts': 'HOUST', 'Corporate_Profits': 'CP',\n",
    "        'Inflation_Rate': 'CPIAUCSL', 'Economic_Policy_Uncertainty': 'USEPUINDXD'\n",
    "    }\n",
    "    fred_data = pd.DataFrame({name: fred.get_series(series_id, observation_start=start_date, observation_end=end_date)\n",
    "                              for name, series_id in fred_series.items()})\n",
    "    return make_tz_naive(fred_data)\n",
    "\n",
    "def get_market_indices(start_date, end_date):\n",
    "    indices = yf.download(['^GSPC', '^VIX'], start=start_date, end=end_date)['Close']\n",
    "    indices.columns = ['SP500', 'VIX']\n",
    "    return make_tz_naive(indices)\n",
    "\n",
    "def get_sector_data(symbol, start_date, end_date):\n",
    "    sector_etfs = {\n",
    "        'Information Technology': 'XLK', 'Health Care': 'XLV', 'Financials': 'XLF',\n",
    "        'Consumer Discretionary': 'XLY', 'Communication Services': 'XLC',\n",
    "        'Industrials': 'XLI', 'Consumer Staples': 'XLP', 'Energy': 'XLE',\n",
    "        'Utilities': 'XLU', 'Real Estate': 'XLRE', 'Materials': 'XLB'\n",
    "    }\n",
    "    stock = yf.Ticker(symbol)\n",
    "    sector = stock.info.get('sector', 'Unknown')  # Use 'Unknown' if sector info is not available\n",
    "    sector_etf = sector_etfs.get(sector, 'SPY')\n",
    "    sector_data = yf.download(sector_etf, start=start_date, end=end_date)['Close']\n",
    "    \n",
    "    # Ensure sector_data is a 1-dimensional Series\n",
    "    sector_data = sector_data.squeeze()  # Convert to Series if it's a DataFrame with a single column\n",
    "    \n",
    "    # Create a DataFrame with the ETF prices\n",
    "    sector_df = pd.DataFrame({\n",
    "        f'{sector}_ETF': sector_data\n",
    "    }, index=sector_data.index)  # Ensure the index is set correctly\n",
    "    \n",
    "    return make_tz_naive(sector_df)\n",
    "\n",
    "# Data Collection\n",
    "print(\"\\nData Collection\")\n",
    "start_date, end_date = '2014-11-11', '2024-11-11'  # Example date range\n",
    "\n",
    "stock_data = get_stock_data(symbol, start_date, end_date)\n",
    "print(stock_data.info())\n",
    "data_with_indicators = calculate_technical_indicators(stock_data)\n",
    "economic_data = get_economic_indicators(fred, start_date, end_date)\n",
    "market_indices = get_market_indices(start_date, end_date)\n",
    "sector_data = get_sector_data(symbol, start_date, end_date)\n",
    "\n",
    "# Prefix columns to avoid overlapping\n",
    "data_with_indicators = data_with_indicators.add_prefix(f'{symbol}_')\n",
    "# economic_data = economic_data.add_prefix('Economic_')\n",
    "# market_indices = market_indices.add_prefix('Market_')\n",
    "# sector_data = sector_data.add_prefix('Sector_')\n",
    "\n",
    "# Join the DataFrames\n",
    "combined_data = data_with_indicators.join([\n",
    "    economic_data,\n",
    "    market_indices,\n",
    "    sector_data,\n",
    "])\n",
    "\n",
    "# Data Preprocessing nan and interpolate, weekend removal\n",
    "def remove_nan(data):\n",
    "    data = data.interpolate().ffill().bfill()\n",
    "    data = data[data.index.dayofweek < 5]  # Remove weekends\n",
    "    print(\"\\nNAN values in data after preprocessing:\\n\", data.isna().sum())\n",
    "    return data\n",
    "\n",
    "# Data Preprocessing\n",
    "cleaned_data = remove_nan(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Business Cycle Indicators\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_cycle_score(row, data):\n",
    "    score = 0\n",
    "    score += 1 if row[f'{symbol}_Close'] > row[f'{symbol}_SMA_20'] else -1\n",
    "    score += 1 if row['GDP'] > data['GDP'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['Industrial_Production'] > data['Industrial_Production'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['Unemployment_Rate'] < data['Unemployment_Rate'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['Consumer_Confidence'] > data['Consumer_Confidence'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['Corporate_Profits'] > data['Corporate_Profits'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['SP500'] > data['SP500'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    return score\n",
    "\n",
    "def determine_business_cycle(score):\n",
    "    if score >= 5:\n",
    "        return 'Expansion'\n",
    "    elif 2 <= score < 5:\n",
    "        return 'Peak'\n",
    "    elif -2 <= score < 2:\n",
    "        return 'Contraction'\n",
    "    else:\n",
    "        return 'Trough'\n",
    "\n",
    "def iterative_bci_assignment(data):\n",
    "    data[f'{symbol}_BCI_Score'] = data.apply(lambda row: calculate_cycle_score(row, data), axis=1)\n",
    "    data[f'{symbol}_Raw_BCI'] = data[f'{symbol}_BCI_Score'].apply(determine_business_cycle)\n",
    "    \n",
    "    bci_periods = []\n",
    "    current_period = data[f'{symbol}_Raw_BCI'].iloc[0]\n",
    "    transition_count = 0\n",
    "    transition_threshold = 20  # Adjust this value as needed\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        if row[f'{symbol}_Raw_BCI'] == current_period:\n",
    "            transition_count = 0\n",
    "        else:\n",
    "            transition_count += 1\n",
    "\n",
    "        if transition_count >= transition_threshold:\n",
    "            if current_period == 'Expansion':\n",
    "                current_period = 'Peak'\n",
    "            elif current_period == 'Peak':\n",
    "                current_period = 'Contraction'\n",
    "            elif current_period == 'Contraction':\n",
    "                current_period = 'Trough'\n",
    "            else:  # Trough\n",
    "                current_period = 'Expansion'\n",
    "            transition_count = 0\n",
    "\n",
    "        bci_periods.append(current_period)\n",
    "\n",
    "    return bci_periods\n",
    "\n",
    "# Calculate Business Cycle Indicators\n",
    "print(\"\\nCalculating Business Cycle Indicator\")\n",
    "cleaned_data[f'{symbol}_Business_Cycle'] = iterative_bci_assignment(cleaned_data)\n",
    "\n",
    "print(\"\\nBusiness Cycle Indicator Distribution:\")\n",
    "print(cleaned_data[f'{symbol}_Business_Cycle'].value_counts(normalize=True))\n",
    "\n",
    "# Calculate Average Timeframes for Each Cycle Phase\n",
    "def calculate_average_timeframes(data):\n",
    "    phase_durations = defaultdict(list)\n",
    "    current_phase = data[f'{symbol}_Business_Cycle'].iloc[0]\n",
    "    phase_start = data.index[0]\n",
    "    \n",
    "    for date, phase in zip(data.index[1:], data[f'{symbol}_Business_Cycle'].iloc[1:]):\n",
    "        if phase != current_phase:\n",
    "            duration = (date - phase_start).days\n",
    "            phase_durations[current_phase].append(duration)\n",
    "            current_phase = phase\n",
    "            phase_start = date\n",
    "    \n",
    "    # Add the last phase\n",
    "    duration = (data.index[-1] - phase_start).days\n",
    "    phase_durations[current_phase].append(duration)\n",
    "    \n",
    "    average_durations = {phase: np.mean(durations) for phase, durations in phase_durations.items()}\n",
    "    return average_durations\n",
    "\n",
    "average_timeframes = calculate_average_timeframes(cleaned_data)\n",
    "print(\"\\nAverage Timeframes for Each Cycle Phase (in days):\")\n",
    "for phase, duration in average_timeframes.items():\n",
    "    print(f\"{phase}: {duration:.2f}\")\n",
    "avg_bc_length = sum(average_timeframes.values())\n",
    "print(f\"\\nAverage Business Cycle Length: {avg_bc_length:.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT Analysis, add statistical features\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import detrend\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "def perform_enhanced_fft_analysis(data):\n",
    "    print(\"\\n--- Enhanced FFT Analysis ---\")\n",
    "    \n",
    "    close_prices = data[f'{symbol}_Close'].values\n",
    "    detrended_prices = detrend(close_prices)\n",
    "    \n",
    "    window = np.hanning(len(detrended_prices))\n",
    "    windowed_prices = detrended_prices * window\n",
    "    fft_result = fft(windowed_prices)\n",
    "    frequencies = np.fft.fftfreq(len(detrended_prices), d=1)\n",
    "    amplitudes = np.abs(fft_result)\n",
    "    \n",
    "    all_cycles = {}\n",
    "    time_scales = [\n",
    "        ('Long-term', 252, 1260),  # 1-5 years\n",
    "        ('Medium-term', 21, 252),  # 1 month to 1 year\n",
    "        ('Short-term', 2, 21)      # 2 days to 1 month\n",
    "    ]\n",
    "    \n",
    "    for scale_name, min_period, max_period in time_scales:\n",
    "        print(f\"\\n{scale_name} Analysis:\")\n",
    "        mask = (1/max_period <= np.abs(frequencies)) & (np.abs(frequencies) <= 1/min_period)\n",
    "        scale_frequencies = frequencies[mask]\n",
    "        scale_amplitudes = amplitudes[mask]\n",
    "        \n",
    "        sorted_indices = np.argsort(scale_amplitudes)[::-1]\n",
    "        top_frequencies = scale_frequencies[sorted_indices[:5]]\n",
    "        top_amplitudes = scale_amplitudes[sorted_indices[:5]]\n",
    "        \n",
    "        print(\"Top 5 dominant frequencies:\")\n",
    "        for i, (freq, amp) in enumerate(zip(top_frequencies, top_amplitudes), 1):\n",
    "            period = 1 / abs(freq) if freq != 0 else np.inf\n",
    "            print(f\"{i}. Frequency: {freq:.6f}, Period: {period:.2f} days, Amplitude: {amp:.2f}\")\n",
    "            \n",
    "            cycle_name = f\"{scale_name.lower()}_cycle_{i}\"\n",
    "            all_cycles[cycle_name] = (freq, amp, period)\n",
    "    \n",
    "    variables_to_compare = ['SP500', 'VIX', 'Sector_Technology_ETF']\n",
    "    for var in variables_to_compare:\n",
    "        if var in data.columns:\n",
    "            var_fft = fft(detrend(data[var].values) * window)\n",
    "            var_amplitudes = np.abs(var_fft)\n",
    "            correlation = np.corrcoef(amplitudes, var_amplitudes)[0, 1]\n",
    "            print(f\"\\nFFT Amplitude correlation with {var}: {correlation:.4f}\")\n",
    "    \n",
    "    return all_cycles\n",
    "\n",
    "def add_statistical_features(data):\n",
    "    print(\"\\n--- Adding Statistical Features ---\")\n",
    "    window_sizes = [5, 10, 20, 50, 100]\n",
    "    new_features = {}\n",
    "    for window in window_sizes:\n",
    "        new_features[f'rolling_mean_{window}'] = data[f'{symbol}_Close'].rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        # For standard deviation, we need at least 2 points\n",
    "        std = data[f'{symbol}_Close'].rolling(window=window, min_periods=2).std()\n",
    "        new_features[f'rolling_std_{window}'] = std.bfill()\n",
    "        \n",
    "        # For skewness, we need at least 3 points\n",
    "        skew = data[f'{symbol}_Close'].rolling(window=window, min_periods=3).skew()\n",
    "        new_features[f'rolling_skew_{window}'] = skew.bfill()\n",
    "        \n",
    "        # For kurtosis, we need at least 4 points\n",
    "        kurt = data[f'{symbol}_Close'].rolling(window=window, min_periods=4).apply(kurtosis)\n",
    "        new_features[f'rolling_kurt_{window}'] = kurt.bfill()\n",
    "        \n",
    "        print(f\"Added rolling statistics for window size {window}\")\n",
    "    \n",
    "    result = pd.concat([data, pd.DataFrame(new_features, index=data.index)], axis=1)\n",
    "    \n",
    "    # Replace any remaining NaNs with the first valid value\n",
    "    result = result.bfill().ffill()\n",
    "    \n",
    "    return result\n",
    "\n",
    "all_cycles = perform_enhanced_fft_analysis(cleaned_data)\n",
    "cleaned_data = add_statistical_features(cleaned_data)\n",
    "print(\"\\nCycle features and statistical features have been added to the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data before split and normalize\n",
    "def analyze_cleaned_data(cleaned_data):\n",
    "    print(\"\\n--- Cleaned Data Analysis ---\")\n",
    "    print(\"\\nData Info:\")\n",
    "    print(cleaned_data.info())\n",
    "    \n",
    "    start_date, end_date = cleaned_data.index.min(), cleaned_data.index.max()\n",
    "    print(f\"\\nCleaned data date range: {start_date} to {end_date}\")\n",
    "    print(f\"Total number of rows: {len(cleaned_data)}\")\n",
    "    print(f\"Number of days between start and end date: {(end_date - start_date).days}\")\n",
    "\n",
    "    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    missing_dates = full_date_range.difference(cleaned_data.index)\n",
    "    print(f\"\\nNumber of missing dates: {len(missing_dates)}\")\n",
    "    if len(missing_dates) > 0:\n",
    "        print(\"First few missing dates:\", missing_dates[:5].tolist())\n",
    "\n",
    "    print(f\"\\nNumber of NaN values in '{symbol}_Close' column: {cleaned_data[f'{symbol}_Close'].isna().sum()}\")\n",
    "\n",
    "    day_counts = cleaned_data.index.dayofweek.value_counts().sort_index()\n",
    "    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    print(\"\\nSummary of days of the week:\")\n",
    "    for day, count in day_counts.items():\n",
    "        print(f\"{day_names[day]}: {count}\")\n",
    "\n",
    "    weekend_data = cleaned_data[cleaned_data.index.dayofweek.isin([5, 6])]\n",
    "    if not weekend_data.empty:\n",
    "        print(\"\\nWarning: Data contains weekend entries:\")\n",
    "        print(weekend_data)\n",
    "\n",
    "    total_weekdays = sum(day.weekday() < 5 for day in full_date_range)\n",
    "    available_weekdays = sum(day_counts[:5])\n",
    "    coverage_percentage = (available_weekdays / total_weekdays) * 100\n",
    "    print(f\"\\nPercentage of available trading days: {coverage_percentage:.2f}%\")\n",
    "\n",
    "analyze_cleaned_data(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import torch\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "categorical_columns = cleaned_data.select_dtypes(include=['object', 'category']).columns\n",
    "numeric_columns = cleaned_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create a ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numeric_columns),\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Fit the preprocessor on the entire dataset\n",
    "preprocessor.fit(cleaned_data)\n",
    "\n",
    "# Transform the data\n",
    "data_transformed = preprocessor.transform(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform correlation analysis on training data only\n",
    "def perform_correlation_analysis(data, column_names, threshold):\n",
    "    print(\"\\n--- Correlation Analysis ---\")\n",
    "    \n",
    "    # Convert the numpy array back to a DataFrame\n",
    "    data_df = pd.DataFrame(data, columns=column_names)\n",
    "    \n",
    "    correlations = data_df.corr()\n",
    "    \n",
    "    # Correlation with target variable (assuming is the target, formatted as num__{symbol}_Close)\n",
    "    # This was from the ColumnTransformer\n",
    "    target_correlations = correlations[f'num__{symbol}_Close'].sort_values(ascending=False)\n",
    "    target_correlations = target_correlations.drop(f'num__{symbol}_Close')\n",
    "    \n",
    "    print(f\"\\nTop 10 Positive Correlations with {symbol}_Close:\")\n",
    "    print(target_correlations.head(10))\n",
    "    print(f\"\\nTop 10 Negative Correlations with {symbol}_Close:\")\n",
    "    print(target_correlations.tail(10))\n",
    "    \n",
    "    # Filter features based on correlation threshold\n",
    "    strong_corr = target_correlations[abs(target_correlations) >= threshold]\n",
    "    weak_corr = target_correlations[abs(target_correlations) < threshold]\n",
    "    \n",
    "    print(f\"\\nFeatures with strong correlation (|r| >= {threshold}): {len(strong_corr)}\")\n",
    "    print(f\"Features with weak correlation (|r| < {threshold}): {len(weak_corr)}\")\n",
    "    \n",
    "    return strong_corr, weak_corr\n",
    "\n",
    "# Assuming you have a list of column names corresponding to the transformed data\n",
    "column_names = preprocessor.get_feature_names_out()\n",
    "print(column_names)\n",
    "strong_corr, weak_corr = perform_correlation_analysis(data_transformed, column_names, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        sequences.append(seq)\n",
    "    # Convert the list of sequences to a numpy array\n",
    "    sequences_array = np.array(sequences)\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    return torch.FloatTensor(sequences_array)\n",
    "\n",
    "seq_length = 60\n",
    "# Get indices of strongly correlated features\n",
    "strong_feature_indices = [i for i, feature in enumerate(column_names) if feature in strong_corr.index]\n",
    "# Select only strongly correlated features from transformed data\n",
    "strong_features_data = data_transformed[:, strong_feature_indices]\n",
    "\n",
    "X = create_sequences(strong_features_data, seq_length)\n",
    "y = torch.FloatTensor(cleaned_data[f'{symbol}_Close'].values[seq_length:])\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "from torch import nn, optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(strong_feature_indices)\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "#! add dropout and learning rate scheduler\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Move data to GPU\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size].view(-1, 1)  # Reshape target to match model output\n",
    "        \n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val.view(-1, 1))  # Reshape target to match model output\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TensorRT-optimized LSTM model (optional)\n",
    "# import torch_tensorrt\n",
    "\n",
    "# # Compile the model with TensorRT\n",
    "# trt_model = torch_tensorrt.compile(model, \n",
    "#     inputs=[torch_tensorrt.Input((batch_size, seq_length, input_size))],\n",
    "#     enabled_precisions={torch.float32, torch.float16} # Enable FP32 and FP16\n",
    "# )\n",
    "\n",
    "# # Save the TensorRT optimized model\n",
    "# torch.jit.save(trt_model, \"trt_lstm_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test.view(-1, 1))  # Ensure target shape matches output\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# Inverse transform predictions\n",
    "# Assuming the MinMaxScaler was applied only to the target variable\n",
    "scaler = preprocessor.named_transformers_['num']  # Access the MinMaxScaler for numeric columns\n",
    "\n",
    "# Inverse transform only the target variable\n",
    "# Select the column index corresponding to the target variable\n",
    "target_index = list(numeric_columns).index(f'{symbol}_Close')\n",
    "predictions = scaler.inverse_transform(np.concatenate([np.zeros((test_outputs.shape[0], target_index)), \n",
    "                                                       test_outputs.cpu().numpy(), \n",
    "                                                       np.zeros((test_outputs.shape[0], len(numeric_columns) - target_index - 1))], axis=1))[:, target_index]\n",
    "actual = scaler.inverse_transform(np.concatenate([np.zeros((y_test.shape[0], target_index)), \n",
    "                                                  y_test.cpu().numpy().reshape(-1, 1), \n",
    "                                                  np.zeros((y_test.shape[0], len(numeric_columns) - target_index - 1))], axis=1))[:, target_index]\n",
    "\n",
    "# Calculate metrics (e.g., RMSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "mae = mean_absolute_error(actual, predictions)\n",
    "\n",
    "# Calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_indices = y_true != 0  # Avoid division by zero\n",
    "    return np.mean(np.abs((y_true[non_zero_indices] - y_pred[non_zero_indices]) / y_true[non_zero_indices])) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(actual, predictions)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
