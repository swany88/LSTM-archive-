{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NAN values in data after preprocessing:\n",
      " QCOM_Open                      0\n",
      "QCOM_High                      0\n",
      "QCOM_Low                       0\n",
      "QCOM_Close                     0\n",
      "QCOM_Volume                    0\n",
      "QCOM_Dividends                 0\n",
      "QCOM_Stock Splits              0\n",
      "QCOM_SMA_20                    0\n",
      "QCOM_EMA_20                    0\n",
      "QCOM_BB_Middle                 0\n",
      "QCOM_BB_Upper                  0\n",
      "QCOM_BB_Lower                  0\n",
      "QCOM_RSI                       0\n",
      "QCOM_MACD                      0\n",
      "QCOM_MACD_Signal               0\n",
      "QCOM_OBV                       0\n",
      "QCOM_ATR                       0\n",
      "GDP                            0\n",
      "Interest_Rates                 0\n",
      "Consumer_Confidence            0\n",
      "Industrial_Production          0\n",
      "Unemployment_Rate              0\n",
      "Retail_Sales                   0\n",
      "Housing_Starts                 0\n",
      "Corporate_Profits              0\n",
      "Inflation_Rate                 0\n",
      "Economic_Policy_Uncertainty    0\n",
      "SP500                          0\n",
      "VIX                            0\n",
      "Technology_ETF                 0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import hashlib\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from fredapi import Fred\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# %% Define stock symbol, api key, functions for retrieving stock data\n",
    "symbol = 'QCOM'\n",
    "\n",
    "# FRED API key\n",
    "fred_api_key = os.getenv('FRED_API_KEY')\n",
    "fred = Fred(api_key=fred_api_key)\n",
    "\n",
    "def make_tz_naive(df):\n",
    "    if df.index.tzinfo is not None:\n",
    "        df.index = df.index.tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def get_stock_data(symbol, start_date, end_date):\n",
    "    return make_tz_naive(yf.Ticker(symbol).history(start=start_date, end=end_date))\n",
    "\n",
    "def calculate_technical_indicators(data):\n",
    "    result = data.copy()\n",
    "    close = result['Close']\n",
    "    result['SMA_20'] = close.rolling(window=20).mean()\n",
    "    result['EMA_20'] = close.ewm(span=20, adjust=False).mean()\n",
    "    result['BB_Middle'] = result['SMA_20']\n",
    "    bb_std = close.rolling(window=20).std()\n",
    "    result['BB_Upper'] = result['BB_Middle'] + 2 * bb_std\n",
    "    result['BB_Lower'] = result['BB_Middle'] - 2 * bb_std\n",
    "    delta = close.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    result['RSI'] = 100 - (100 / (1 + rs))\n",
    "    ema_12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = close.ewm(span=26, adjust=False).mean()\n",
    "    result['MACD'] = ema_12 - ema_26\n",
    "    result['MACD_Signal'] = result['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    result['OBV'] = (np.sign(delta) * result['Volume']).fillna(0).cumsum()\n",
    "\n",
    "    #? New volatility indicator: Average True Range (ATR)\n",
    "    tr1 = result['High'] - result['Low']\n",
    "    tr2 = abs(result['High'] - result['Close'].shift())\n",
    "    tr3 = abs(result['Low'] - result['Close'].shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    result['ATR'] = tr.rolling(window=14).mean()\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_economic_indicators(fred, start_date, end_date):\n",
    "    fred_series = {\n",
    "        'GDP': 'GDP', 'Interest_Rates': 'FEDFUNDS', 'Consumer_Confidence': 'UMCSENT',\n",
    "        'Industrial_Production': 'INDPRO', 'Unemployment_Rate': 'UNRATE',\n",
    "        'Retail_Sales': 'RSAFS', 'Housing_Starts': 'HOUST', 'Corporate_Profits': 'CP',\n",
    "        'Inflation_Rate': 'CPIAUCSL', 'Economic_Policy_Uncertainty': 'USEPUINDXD'\n",
    "    }\n",
    "    fred_data = pd.DataFrame({name: fred.get_series(series_id, observation_start=start_date, observation_end=end_date)\n",
    "                              for name, series_id in fred_series.items()})\n",
    "    return make_tz_naive(fred_data)\n",
    "\n",
    "def get_market_indices(start_date, end_date):\n",
    "    indices = yf.download(['^GSPC', '^VIX'], start=start_date, end=end_date)['Close']\n",
    "    indices.columns = ['SP500', 'VIX']\n",
    "    return make_tz_naive(indices)\n",
    "\n",
    "def get_sector_data(symbol, start_date, end_date):\n",
    "    sector_etfs = {\n",
    "        'Information Technology': 'XLK', 'Health Care': 'XLV', 'Financials': 'XLF',\n",
    "        'Consumer Discretionary': 'XLY', 'Communication Services': 'XLC',\n",
    "        'Industrials': 'XLI', 'Consumer Staples': 'XLP', 'Energy': 'XLE',\n",
    "        'Utilities': 'XLU', 'Real Estate': 'XLRE', 'Materials': 'XLB'\n",
    "    }\n",
    "    stock = yf.Ticker(symbol)\n",
    "    sector = stock.info.get('sector', 'Unknown')  # Use 'Unknown' if sector info is not available\n",
    "    sector_etf = sector_etfs.get(sector, 'SPY')\n",
    "    sector_data = yf.download(sector_etf, start=start_date, end=end_date)['Close']\n",
    "    \n",
    "    # Ensure sector_data is a 1-dimensional Series\n",
    "    sector_data = sector_data.squeeze()  # Convert to Series if it's a DataFrame with a single column\n",
    "    \n",
    "    # Create a DataFrame with the ETF prices\n",
    "    sector_df = pd.DataFrame({\n",
    "        f'{sector}_ETF': sector_data\n",
    "    }, index=sector_data.index)  # Ensure the index is set correctly\n",
    "    \n",
    "    return make_tz_naive(sector_df)\n",
    "\n",
    "# Data Collection\n",
    "print(\"\\nData Collection\")\n",
    "start_date, end_date = '2014-11-11', '2024-11-11'  # Example date range\n",
    "\n",
    "stock_data = get_stock_data(symbol, start_date, end_date)\n",
    "data_with_indicators = calculate_technical_indicators(stock_data)\n",
    "economic_data = get_economic_indicators(fred, start_date, end_date)\n",
    "market_indices = get_market_indices(start_date, end_date)\n",
    "sector_data = get_sector_data(symbol, start_date, end_date)\n",
    "\n",
    "# Prefix columns to avoid overlapping\n",
    "data_with_indicators = data_with_indicators.add_prefix(f'{symbol}_')\n",
    "# economic_data = economic_data.add_prefix('Economic_')\n",
    "# market_indices = market_indices.add_prefix('Market_')\n",
    "# sector_data = sector_data.add_prefix('Sector_')\n",
    "\n",
    "# Join the DataFrames\n",
    "combined_data = data_with_indicators.join([\n",
    "    economic_data,\n",
    "    market_indices,\n",
    "    sector_data,\n",
    "])\n",
    "\n",
    "# Data Preprocessing nan and interpolate, weekend removal\n",
    "def remove_nan(data):\n",
    "    data = data.interpolate().ffill().bfill()\n",
    "    data = data[data.index.dayofweek < 5]  # Remove weekends\n",
    "    print(\"\\nNAN values in data after preprocessing:\\n\", data.isna().sum())\n",
    "    return data\n",
    "\n",
    "# Data Preprocessing\n",
    "cleaned_data = remove_nan(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating Business Cycle Indicator\n",
      "\n",
      "Business Cycle Indicator Distribution:\n",
      "QCOM_Business_Cycle\n",
      "Contraction    0.422099\n",
      "Peak           0.234102\n",
      "Expansion      0.201113\n",
      "Trough         0.142687\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Average Timeframes for Each Cycle Phase (in days):\n",
      "Trough: 48.18\n",
      "Expansion: 66.82\n",
      "Peak: 77.09\n",
      "Contraction: 139.73\n",
      "\n",
      "Average Business Cycle Length: 331.82 days\n"
     ]
    }
   ],
   "source": [
    "# Calculate Business Cycle Indicators\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_cycle_score(row, data):\n",
    "    score = 0\n",
    "    score += 1 if row[f'{symbol}_Close'] > row[f'{symbol}_SMA_20'] else -1\n",
    "    score += 1 if row['GDP'] > data['GDP'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['Industrial_Production'] > data['Industrial_Production'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['Unemployment_Rate'] < data['Unemployment_Rate'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['Consumer_Confidence'] > data['Consumer_Confidence'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['Corporate_Profits'] > data['Corporate_Profits'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    score += 1 if row['SP500'] > data['SP500'].rolling(window=20).mean().loc[row.name] else -1\n",
    "    return score\n",
    "\n",
    "def determine_business_cycle(score):\n",
    "    if score >= 5:\n",
    "        return 'Expansion'\n",
    "    elif 2 <= score < 5:\n",
    "        return 'Peak'\n",
    "    elif -2 <= score < 2:\n",
    "        return 'Contraction'\n",
    "    else:\n",
    "        return 'Trough'\n",
    "\n",
    "def iterative_bci_assignment(data):\n",
    "    data[f'{symbol}_BCI_Score'] = data.apply(lambda row: calculate_cycle_score(row, data), axis=1)\n",
    "    data[f'{symbol}_Raw_BCI'] = data[f'{symbol}_BCI_Score'].apply(determine_business_cycle)\n",
    "    \n",
    "    bci_periods = []\n",
    "    current_period = data[f'{symbol}_Raw_BCI'].iloc[0]\n",
    "    transition_count = 0\n",
    "    transition_threshold = 20  # Adjust this value as needed\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        if row[f'{symbol}_Raw_BCI'] == current_period:\n",
    "            transition_count = 0\n",
    "        else:\n",
    "            transition_count += 1\n",
    "\n",
    "        if transition_count >= transition_threshold:\n",
    "            if current_period == 'Expansion':\n",
    "                current_period = 'Peak'\n",
    "            elif current_period == 'Peak':\n",
    "                current_period = 'Contraction'\n",
    "            elif current_period == 'Contraction':\n",
    "                current_period = 'Trough'\n",
    "            else:  # Trough\n",
    "                current_period = 'Expansion'\n",
    "            transition_count = 0\n",
    "\n",
    "        bci_periods.append(current_period)\n",
    "\n",
    "    return bci_periods\n",
    "\n",
    "# Calculate Business Cycle Indicators\n",
    "print(\"\\nCalculating Business Cycle Indicator\")\n",
    "cleaned_data[f'{symbol}_Business_Cycle'] = iterative_bci_assignment(cleaned_data)\n",
    "\n",
    "print(\"\\nBusiness Cycle Indicator Distribution:\")\n",
    "print(cleaned_data[f'{symbol}_Business_Cycle'].value_counts(normalize=True))\n",
    "\n",
    "# Calculate Average Timeframes for Each Cycle Phase\n",
    "def calculate_average_timeframes(data):\n",
    "    phase_durations = defaultdict(list)\n",
    "    current_phase = data[f'{symbol}_Business_Cycle'].iloc[0]\n",
    "    phase_start = data.index[0]\n",
    "    \n",
    "    for date, phase in zip(data.index[1:], data[f'{symbol}_Business_Cycle'].iloc[1:]):\n",
    "        if phase != current_phase:\n",
    "            duration = (date - phase_start).days\n",
    "            phase_durations[current_phase].append(duration)\n",
    "            current_phase = phase\n",
    "            phase_start = date\n",
    "    \n",
    "    # Add the last phase\n",
    "    duration = (data.index[-1] - phase_start).days\n",
    "    phase_durations[current_phase].append(duration)\n",
    "    \n",
    "    average_durations = {phase: np.mean(durations) for phase, durations in phase_durations.items()}\n",
    "    return average_durations\n",
    "\n",
    "average_timeframes = calculate_average_timeframes(cleaned_data)\n",
    "print(\"\\nAverage Timeframes for Each Cycle Phase (in days):\")\n",
    "for phase, duration in average_timeframes.items():\n",
    "    print(f\"{phase}: {duration:.2f}\")\n",
    "avg_bc_length = sum(average_timeframes.values())\n",
    "print(f\"\\nAverage Business Cycle Length: {avg_bc_length:.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Enhanced FFT Analysis ---\n",
      "\n",
      "Long-term Analysis:\n",
      "Top 5 dominant frequencies:\n",
      "1. Frequency: -0.000795, Period: 1258.00 days, Amplitude: 13997.37\n",
      "2. Frequency: 0.000795, Period: 1258.00 days, Amplitude: 13997.37\n",
      "3. Frequency: -0.001192, Period: 838.67 days, Amplitude: 7771.23\n",
      "4. Frequency: 0.001192, Period: 838.67 days, Amplitude: 7771.23\n",
      "5. Frequency: -0.003180, Period: 314.50 days, Amplitude: 4718.78\n",
      "\n",
      "Medium-term Analysis:\n",
      "Top 5 dominant frequencies:\n",
      "1. Frequency: -0.003975, Period: 251.60 days, Amplitude: 3550.36\n",
      "2. Frequency: 0.003975, Period: 251.60 days, Amplitude: 3550.36\n",
      "3. Frequency: -0.005962, Period: 167.73 days, Amplitude: 2644.65\n",
      "4. Frequency: 0.005962, Period: 167.73 days, Amplitude: 2644.65\n",
      "5. Frequency: -0.007552, Period: 132.42 days, Amplitude: 2473.82\n",
      "\n",
      "Short-term Analysis:\n",
      "Top 5 dominant frequencies:\n",
      "1. Frequency: 0.058426, Period: 17.12 days, Amplitude: 518.34\n",
      "2. Frequency: -0.058426, Period: 17.12 days, Amplitude: 518.34\n",
      "3. Frequency: 0.058824, Period: 17.00 days, Amplitude: 403.88\n",
      "4. Frequency: -0.058824, Period: 17.00 days, Amplitude: 403.88\n",
      "5. Frequency: -0.060811, Period: 16.44 days, Amplitude: 372.21\n",
      "\n",
      "FFT Amplitude correlation with SP500: 0.8840\n",
      "\n",
      "FFT Amplitude correlation with VIX: 0.7499\n",
      "\n",
      "--- Adding Statistical Features ---\n",
      "Added rolling statistics for window size 5\n",
      "Added rolling statistics for window size 10\n",
      "Added rolling statistics for window size 20\n",
      "Added rolling statistics for window size 50\n",
      "Added rolling statistics for window size 100\n",
      "\n",
      "Cycle features and statistical features have been added to the dataframe.\n"
     ]
    }
   ],
   "source": [
    "# FFT Analysis, add statistical features\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import detrend\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "def perform_enhanced_fft_analysis(data):\n",
    "    print(\"\\n--- Enhanced FFT Analysis ---\")\n",
    "    \n",
    "    close_prices = data[f'{symbol}_Close'].values\n",
    "    detrended_prices = detrend(close_prices)\n",
    "    \n",
    "    window = np.hanning(len(detrended_prices))\n",
    "    windowed_prices = detrended_prices * window\n",
    "    fft_result = fft(windowed_prices)\n",
    "    frequencies = np.fft.fftfreq(len(detrended_prices), d=1)\n",
    "    amplitudes = np.abs(fft_result)\n",
    "    \n",
    "    all_cycles = {}\n",
    "    time_scales = [\n",
    "        ('Long-term', 252, 1260),  # 1-5 years\n",
    "        ('Medium-term', 21, 252),  # 1 month to 1 year\n",
    "        ('Short-term', 2, 21)      # 2 days to 1 month\n",
    "    ]\n",
    "    \n",
    "    for scale_name, min_period, max_period in time_scales:\n",
    "        print(f\"\\n{scale_name} Analysis:\")\n",
    "        mask = (1/max_period <= np.abs(frequencies)) & (np.abs(frequencies) <= 1/min_period)\n",
    "        scale_frequencies = frequencies[mask]\n",
    "        scale_amplitudes = amplitudes[mask]\n",
    "        \n",
    "        sorted_indices = np.argsort(scale_amplitudes)[::-1]\n",
    "        top_frequencies = scale_frequencies[sorted_indices[:5]]\n",
    "        top_amplitudes = scale_amplitudes[sorted_indices[:5]]\n",
    "        \n",
    "        print(\"Top 5 dominant frequencies:\")\n",
    "        for i, (freq, amp) in enumerate(zip(top_frequencies, top_amplitudes), 1):\n",
    "            period = 1 / abs(freq) if freq != 0 else np.inf\n",
    "            print(f\"{i}. Frequency: {freq:.6f}, Period: {period:.2f} days, Amplitude: {amp:.2f}\")\n",
    "            \n",
    "            cycle_name = f\"{scale_name.lower()}_cycle_{i}\"\n",
    "            all_cycles[cycle_name] = (freq, amp, period)\n",
    "    \n",
    "    variables_to_compare = ['SP500', 'VIX', 'Sector_Technology_ETF']\n",
    "    for var in variables_to_compare:\n",
    "        if var in data.columns:\n",
    "            var_fft = fft(detrend(data[var].values) * window)\n",
    "            var_amplitudes = np.abs(var_fft)\n",
    "            correlation = np.corrcoef(amplitudes, var_amplitudes)[0, 1]\n",
    "            print(f\"\\nFFT Amplitude correlation with {var}: {correlation:.4f}\")\n",
    "    \n",
    "    return all_cycles\n",
    "\n",
    "def add_statistical_features(data):\n",
    "    print(\"\\n--- Adding Statistical Features ---\")\n",
    "    window_sizes = [5, 10, 20, 50, 100]\n",
    "    new_features = {}\n",
    "    for window in window_sizes:\n",
    "        new_features[f'rolling_mean_{window}'] = data[f'{symbol}_Close'].rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        # For standard deviation, we need at least 2 points\n",
    "        std = data[f'{symbol}_Close'].rolling(window=window, min_periods=2).std()\n",
    "        new_features[f'rolling_std_{window}'] = std.bfill()\n",
    "        \n",
    "        # For skewness, we need at least 3 points\n",
    "        skew = data[f'{symbol}_Close'].rolling(window=window, min_periods=3).skew()\n",
    "        new_features[f'rolling_skew_{window}'] = skew.bfill()\n",
    "        \n",
    "        # For kurtosis, we need at least 4 points\n",
    "        kurt = data[f'{symbol}_Close'].rolling(window=window, min_periods=4).apply(kurtosis)\n",
    "        new_features[f'rolling_kurt_{window}'] = kurt.bfill()\n",
    "        \n",
    "        print(f\"Added rolling statistics for window size {window}\")\n",
    "    \n",
    "    result = pd.concat([data, pd.DataFrame(new_features, index=data.index)], axis=1)\n",
    "    \n",
    "    # Replace any remaining NaNs with the first valid value\n",
    "    result = result.bfill().ffill()\n",
    "    \n",
    "    return result\n",
    "\n",
    "all_cycles = perform_enhanced_fft_analysis(cleaned_data)\n",
    "cleaned_data = add_statistical_features(cleaned_data)\n",
    "print(\"\\nCycle features and statistical features have been added to the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleaned Data Analysis ---\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2516 entries, 2014-11-11 to 2024-11-08\n",
      "Data columns (total 53 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   QCOM_Open                    2516 non-null   float64\n",
      " 1   QCOM_High                    2516 non-null   float64\n",
      " 2   QCOM_Low                     2516 non-null   float64\n",
      " 3   QCOM_Close                   2516 non-null   float64\n",
      " 4   QCOM_Volume                  2516 non-null   float64\n",
      " 5   QCOM_Dividends               2516 non-null   float64\n",
      " 6   QCOM_Stock Splits            2516 non-null   float64\n",
      " 7   QCOM_SMA_20                  2516 non-null   float64\n",
      " 8   QCOM_EMA_20                  2516 non-null   float64\n",
      " 9   QCOM_BB_Middle               2516 non-null   float64\n",
      " 10  QCOM_BB_Upper                2516 non-null   float64\n",
      " 11  QCOM_BB_Lower                2516 non-null   float64\n",
      " 12  QCOM_RSI                     2516 non-null   float64\n",
      " 13  QCOM_MACD                    2516 non-null   float64\n",
      " 14  QCOM_MACD_Signal             2516 non-null   float64\n",
      " 15  QCOM_OBV                     2516 non-null   float64\n",
      " 16  QCOM_ATR                     2516 non-null   float64\n",
      " 17  GDP                          2516 non-null   float64\n",
      " 18  Interest_Rates               2516 non-null   float64\n",
      " 19  Consumer_Confidence          2516 non-null   float64\n",
      " 20  Industrial_Production        2516 non-null   float64\n",
      " 21  Unemployment_Rate            2516 non-null   float64\n",
      " 22  Retail_Sales                 2516 non-null   float64\n",
      " 23  Housing_Starts               2516 non-null   float64\n",
      " 24  Corporate_Profits            2516 non-null   float64\n",
      " 25  Inflation_Rate               2516 non-null   float64\n",
      " 26  Economic_Policy_Uncertainty  2516 non-null   float64\n",
      " 27  SP500                        2516 non-null   float64\n",
      " 28  VIX                          2516 non-null   float64\n",
      " 29  Technology_ETF               2516 non-null   float64\n",
      " 30  QCOM_BCI_Score               2516 non-null   int64  \n",
      " 31  QCOM_Raw_BCI                 2516 non-null   object \n",
      " 32  QCOM_Business_Cycle          2516 non-null   object \n",
      " 33  rolling_mean_5               2516 non-null   float64\n",
      " 34  rolling_std_5                2516 non-null   float64\n",
      " 35  rolling_skew_5               2516 non-null   float64\n",
      " 36  rolling_kurt_5               2516 non-null   float64\n",
      " 37  rolling_mean_10              2516 non-null   float64\n",
      " 38  rolling_std_10               2516 non-null   float64\n",
      " 39  rolling_skew_10              2516 non-null   float64\n",
      " 40  rolling_kurt_10              2516 non-null   float64\n",
      " 41  rolling_mean_20              2516 non-null   float64\n",
      " 42  rolling_std_20               2516 non-null   float64\n",
      " 43  rolling_skew_20              2516 non-null   float64\n",
      " 44  rolling_kurt_20              2516 non-null   float64\n",
      " 45  rolling_mean_50              2516 non-null   float64\n",
      " 46  rolling_std_50               2516 non-null   float64\n",
      " 47  rolling_skew_50              2516 non-null   float64\n",
      " 48  rolling_kurt_50              2516 non-null   float64\n",
      " 49  rolling_mean_100             2516 non-null   float64\n",
      " 50  rolling_std_100              2516 non-null   float64\n",
      " 51  rolling_skew_100             2516 non-null   float64\n",
      " 52  rolling_kurt_100             2516 non-null   float64\n",
      "dtypes: float64(50), int64(1), object(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "\n",
      "Cleaned data date range: 2014-11-11 00:00:00 to 2024-11-08 00:00:00\n",
      "Total number of rows: 2516\n",
      "Number of days between start and end date: 3650\n",
      "\n",
      "Number of missing dates: 1135\n",
      "First few missing dates: [Timestamp('2014-11-15 00:00:00'), Timestamp('2014-11-16 00:00:00'), Timestamp('2014-11-22 00:00:00'), Timestamp('2014-11-23 00:00:00'), Timestamp('2014-11-27 00:00:00')]\n",
      "\n",
      "Number of NaN values in 'QCOM_Close' column: 0\n",
      "\n",
      "Summary of days of the week:\n",
      "Monday: 468\n",
      "Tuesday: 518\n",
      "Wednesday: 517\n",
      "Thursday: 508\n",
      "Friday: 505\n",
      "\n",
      "Percentage of available trading days: 96.44%\n"
     ]
    }
   ],
   "source": [
    "# Check data before split and normalize\n",
    "def analyze_cleaned_data(cleaned_data):\n",
    "    print(\"\\n--- Cleaned Data Analysis ---\")\n",
    "    print(\"\\nData Info:\")\n",
    "    print(cleaned_data.info())\n",
    "    \n",
    "    start_date, end_date = cleaned_data.index.min(), cleaned_data.index.max()\n",
    "    print(f\"\\nCleaned data date range: {start_date} to {end_date}\")\n",
    "    print(f\"Total number of rows: {len(cleaned_data)}\")\n",
    "    print(f\"Number of days between start and end date: {(end_date - start_date).days}\")\n",
    "\n",
    "    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    missing_dates = full_date_range.difference(cleaned_data.index)\n",
    "    print(f\"\\nNumber of missing dates: {len(missing_dates)}\")\n",
    "    if len(missing_dates) > 0:\n",
    "        print(\"First few missing dates:\", missing_dates[:5].tolist())\n",
    "\n",
    "    print(f\"\\nNumber of NaN values in '{symbol}_Close' column: {cleaned_data[f'{symbol}_Close'].isna().sum()}\")\n",
    "\n",
    "    day_counts = cleaned_data.index.dayofweek.value_counts().sort_index()\n",
    "    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    print(\"\\nSummary of days of the week:\")\n",
    "    for day, count in day_counts.items():\n",
    "        print(f\"{day_names[day]}: {count}\")\n",
    "\n",
    "    weekend_data = cleaned_data[cleaned_data.index.dayofweek.isin([5, 6])]\n",
    "    if not weekend_data.empty:\n",
    "        print(\"\\nWarning: Data contains weekend entries:\")\n",
    "        print(weekend_data)\n",
    "\n",
    "    total_weekdays = sum(day.weekday() < 5 for day in full_date_range)\n",
    "    available_weekdays = sum(day_counts[:5])\n",
    "    coverage_percentage = (available_weekdays / total_weekdays) * 100\n",
    "    print(f\"\\nPercentage of available trading days: {coverage_percentage:.2f}%\")\n",
    "\n",
    "analyze_cleaned_data(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import torch\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "categorical_columns = cleaned_data.select_dtypes(include=['object', 'category']).columns\n",
    "numeric_columns = cleaned_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create a ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numeric_columns),\n",
    "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Fit the preprocessor on the entire dataset\n",
    "preprocessor.fit(cleaned_data)\n",
    "\n",
    "# Transform the data\n",
    "data_transformed = preprocessor.transform(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num__QCOM_Open' 'num__QCOM_High' 'num__QCOM_Low' 'num__QCOM_Close'\n",
      " 'num__QCOM_Volume' 'num__QCOM_Dividends' 'num__QCOM_Stock Splits'\n",
      " 'num__QCOM_SMA_20' 'num__QCOM_EMA_20' 'num__QCOM_BB_Middle'\n",
      " 'num__QCOM_BB_Upper' 'num__QCOM_BB_Lower' 'num__QCOM_RSI'\n",
      " 'num__QCOM_MACD' 'num__QCOM_MACD_Signal' 'num__QCOM_OBV' 'num__QCOM_ATR'\n",
      " 'num__GDP' 'num__Interest_Rates' 'num__Consumer_Confidence'\n",
      " 'num__Industrial_Production' 'num__Unemployment_Rate' 'num__Retail_Sales'\n",
      " 'num__Housing_Starts' 'num__Corporate_Profits' 'num__Inflation_Rate'\n",
      " 'num__Economic_Policy_Uncertainty' 'num__SP500' 'num__VIX'\n",
      " 'num__Technology_ETF' 'num__QCOM_BCI_Score' 'num__rolling_mean_5'\n",
      " 'num__rolling_std_5' 'num__rolling_skew_5' 'num__rolling_kurt_5'\n",
      " 'num__rolling_mean_10' 'num__rolling_std_10' 'num__rolling_skew_10'\n",
      " 'num__rolling_kurt_10' 'num__rolling_mean_20' 'num__rolling_std_20'\n",
      " 'num__rolling_skew_20' 'num__rolling_kurt_20' 'num__rolling_mean_50'\n",
      " 'num__rolling_std_50' 'num__rolling_skew_50' 'num__rolling_kurt_50'\n",
      " 'num__rolling_mean_100' 'num__rolling_std_100' 'num__rolling_skew_100'\n",
      " 'num__rolling_kurt_100' 'cat__QCOM_Raw_BCI_Contraction'\n",
      " 'cat__QCOM_Raw_BCI_Expansion' 'cat__QCOM_Raw_BCI_Peak'\n",
      " 'cat__QCOM_Raw_BCI_Trough' 'cat__QCOM_Business_Cycle_Contraction'\n",
      " 'cat__QCOM_Business_Cycle_Expansion' 'cat__QCOM_Business_Cycle_Peak'\n",
      " 'cat__QCOM_Business_Cycle_Trough']\n",
      "\n",
      "--- Correlation Analysis ---\n",
      "\n",
      "Top 10 Positive Correlations with QCOM_Close:\n",
      "num__QCOM_Low           0.999643\n",
      "num__QCOM_High          0.999581\n",
      "num__QCOM_Open          0.999132\n",
      "num__rolling_mean_5     0.998429\n",
      "num__rolling_mean_10    0.996387\n",
      "num__QCOM_EMA_20        0.994301\n",
      "num__rolling_mean_20    0.992221\n",
      "num__QCOM_BB_Middle     0.992220\n",
      "num__QCOM_SMA_20        0.992220\n",
      "num__QCOM_BB_Upper      0.990795\n",
      "Name: num__QCOM_Close, dtype: float64\n",
      "\n",
      "Top 10 Negative Correlations with QCOM_Close:\n",
      "cat__QCOM_Raw_BCI_Trough               -0.029635\n",
      "num__rolling_kurt_10                   -0.036294\n",
      "num__rolling_kurt_50                   -0.049519\n",
      "cat__QCOM_Business_Cycle_Contraction   -0.057474\n",
      "cat__QCOM_Business_Cycle_Trough        -0.102947\n",
      "num__Unemployment_Rate                 -0.116650\n",
      "num__rolling_kurt_100                  -0.160870\n",
      "num__QCOM_Volume                       -0.177181\n",
      "num__Consumer_Confidence               -0.783541\n",
      "num__QCOM_Stock Splits                       NaN\n",
      "Name: num__QCOM_Close, dtype: float64\n",
      "\n",
      "Features with strong correlation (|r| >= 0.5): 28\n",
      "Features with weak correlation (|r| < 0.5): 29\n"
     ]
    }
   ],
   "source": [
    "# Perform correlation analysis on training data only\n",
    "def perform_correlation_analysis(data, column_names, threshold):\n",
    "    print(\"\\n--- Correlation Analysis ---\")\n",
    "    \n",
    "    # Convert the numpy array back to a DataFrame\n",
    "    data_df = pd.DataFrame(data, columns=column_names)\n",
    "    \n",
    "    correlations = data_df.corr()\n",
    "    \n",
    "    # Correlation with target variable (assuming is the target, formatted as num__{symbol}_Close)\n",
    "    # This was from the ColumnTransformer\n",
    "    target_correlations = correlations[f'num__{symbol}_Close'].sort_values(ascending=False)\n",
    "    target_correlations = target_correlations.drop(f'num__{symbol}_Close')\n",
    "    \n",
    "    print(f\"\\nTop 10 Positive Correlations with {symbol}_Close:\")\n",
    "    print(target_correlations.head(10))\n",
    "    print(f\"\\nTop 10 Negative Correlations with {symbol}_Close:\")\n",
    "    print(target_correlations.tail(10))\n",
    "    \n",
    "    # Filter features based on correlation threshold\n",
    "    strong_corr = target_correlations[abs(target_correlations) >= threshold]\n",
    "    weak_corr = target_correlations[abs(target_correlations) < threshold]\n",
    "    \n",
    "    print(f\"\\nFeatures with strong correlation (|r| >= {threshold}): {len(strong_corr)}\")\n",
    "    print(f\"Features with weak correlation (|r| < {threshold}): {len(weak_corr)}\")\n",
    "    \n",
    "    return strong_corr, weak_corr\n",
    "\n",
    "# Assuming you have a list of column names corresponding to the transformed data\n",
    "column_names = preprocessor.get_feature_names_out()\n",
    "print(column_names)\n",
    "strong_corr, weak_corr = perform_correlation_analysis(data_transformed, column_names, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        sequences.append(seq)\n",
    "    # Convert the list of sequences to a numpy array\n",
    "    sequences_array = np.array(sequences)\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    return torch.FloatTensor(sequences_array)\n",
    "\n",
    "seq_length = 60\n",
    "# Get indices of strongly correlated features\n",
    "strong_feature_indices = [i for i, feature in enumerate(column_names) if feature in strong_corr.index]\n",
    "# Select only strongly correlated features from transformed data\n",
    "strong_features_data = data_transformed[:, strong_feature_indices]\n",
    "\n",
    "X = create_sequences(strong_features_data, seq_length)\n",
    "y = torch.FloatTensor(cleaned_data[f'{symbol}_Close'].values[seq_length:])\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "from torch import nn, optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(strong_feature_indices)\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "#! add dropout and learning rate scheduler\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Move data to GPU\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 14134.6855, Val Loss: 16913.3594\n",
      "Epoch [2/100], Loss: 13279.3076, Val Loss: 15988.8496\n",
      "Epoch [3/100], Loss: 12574.7637, Val Loss: 15224.0576\n",
      "Epoch [4/100], Loss: 11938.4160, Val Loss: 14531.3867\n",
      "Epoch [5/100], Loss: 11351.7363, Val Loss: 13891.2744\n",
      "Epoch [6/100], Loss: 10806.6904, Val Loss: 13295.2656\n",
      "Epoch [7/100], Loss: 10298.6162, Val Loss: 12738.4736\n",
      "Epoch [8/100], Loss: 9824.2832, Val Loss: 12217.5371\n",
      "Epoch [9/100], Loss: 9381.1953, Val Loss: 11729.8730\n",
      "Epoch [10/100], Loss: 8967.2812, Val Loss: 11273.3369\n",
      "Epoch [11/100], Loss: 8580.7295, Val Loss: 10846.0762\n",
      "Epoch [12/100], Loss: 8219.9121, Val Loss: 10446.4150\n",
      "Epoch [13/100], Loss: 7883.3369, Val Loss: 10072.8252\n",
      "Epoch [14/100], Loss: 7569.6147, Val Loss: 9723.8740\n",
      "Epoch [15/100], Loss: 7277.4365, Val Loss: 9398.2188\n",
      "Epoch [16/100], Loss: 7005.5679, Val Loss: 9094.5869\n",
      "Epoch [17/100], Loss: 6752.8335, Val Loss: 8811.7598\n",
      "Epoch [18/100], Loss: 6518.1152, Val Loss: 8548.5840\n",
      "Epoch [19/100], Loss: 6300.3477, Val Loss: 8303.9482\n",
      "Epoch [20/100], Loss: 6098.5132, Val Loss: 8076.7876\n",
      "Epoch [21/100], Loss: 5911.6396, Val Loss: 7866.0840\n",
      "Epoch [22/100], Loss: 5738.7983, Val Loss: 7670.8628\n",
      "Epoch [23/100], Loss: 5579.1030, Val Loss: 7490.1855\n",
      "Epoch [24/100], Loss: 5431.7158, Val Loss: 7323.1592\n",
      "Epoch [25/100], Loss: 5295.8276, Val Loss: 7168.9238\n",
      "Epoch [26/100], Loss: 5170.6777, Val Loss: 7026.6641\n",
      "Epoch [27/100], Loss: 5055.5386, Val Loss: 6895.5967\n",
      "Epoch [28/100], Loss: 4949.7217, Val Loss: 6774.9746\n",
      "Epoch [29/100], Loss: 4852.5757, Val Loss: 6664.0952\n",
      "Epoch [30/100], Loss: 4763.4824, Val Loss: 6562.2822\n",
      "Epoch [31/100], Loss: 4681.8623, Val Loss: 6468.8994\n",
      "Epoch [32/100], Loss: 4607.1631, Val Loss: 6383.3418\n",
      "Epoch [33/100], Loss: 4538.8691, Val Loss: 6305.0391\n",
      "Epoch [34/100], Loss: 4476.4951, Val Loss: 6233.4536\n",
      "Epoch [35/100], Loss: 4419.5830, Val Loss: 6168.0791\n",
      "Epoch [36/100], Loss: 4367.7065, Val Loss: 6108.4365\n",
      "Epoch [37/100], Loss: 4320.4683, Val Loss: 6054.0815\n",
      "Epoch [38/100], Loss: 4277.4897, Val Loss: 6004.5928\n",
      "Epoch [39/100], Loss: 4238.4292, Val Loss: 5959.5801\n",
      "Epoch [40/100], Loss: 4202.9590, Val Loss: 5918.6807\n",
      "Epoch [41/100], Loss: 4170.7798, Val Loss: 5881.5508\n",
      "Epoch [42/100], Loss: 4141.6108, Val Loss: 5847.8765\n",
      "Epoch [43/100], Loss: 4115.1963, Val Loss: 5817.3613\n",
      "Epoch [44/100], Loss: 4091.2935, Val Loss: 5789.7368\n",
      "Epoch [45/100], Loss: 4069.6833, Val Loss: 5764.7456\n",
      "Epoch [46/100], Loss: 4050.1602, Val Loss: 5742.1602\n",
      "Epoch [47/100], Loss: 4032.5400, Val Loss: 5721.7642\n",
      "Epoch [48/100], Loss: 4016.6455, Val Loss: 5703.3589\n",
      "Epoch [49/100], Loss: 4002.3220, Val Loss: 5686.7632\n",
      "Epoch [50/100], Loss: 3989.4199, Val Loss: 5671.8125\n",
      "Epoch [51/100], Loss: 3977.8093, Val Loss: 5658.3501\n",
      "Epoch [52/100], Loss: 3967.3687, Val Loss: 5646.2378\n",
      "Epoch [53/100], Loss: 3957.9841, Val Loss: 5635.3472\n",
      "Epoch [54/100], Loss: 3949.5566, Val Loss: 5625.5625\n",
      "Epoch [55/100], Loss: 3941.9912, Val Loss: 5616.7759\n",
      "Epoch [56/100], Loss: 3935.2078, Val Loss: 5608.8926\n",
      "Epoch [57/100], Loss: 3929.1230, Val Loss: 5601.8198\n",
      "Epoch [58/100], Loss: 3923.6726, Val Loss: 5595.4814\n",
      "Epoch [59/100], Loss: 3918.7927, Val Loss: 5589.8022\n",
      "Epoch [60/100], Loss: 3914.4253, Val Loss: 5584.7168\n",
      "Epoch [61/100], Loss: 3910.5190, Val Loss: 5580.1665\n",
      "Epoch [62/100], Loss: 3907.0261, Val Loss: 5576.0962\n",
      "Epoch [63/100], Loss: 3903.9060, Val Loss: 5572.4575\n",
      "Epoch [64/100], Loss: 3901.1191, Val Loss: 5569.2036\n",
      "Epoch [65/100], Loss: 3898.6306, Val Loss: 5566.2988\n",
      "Epoch [66/100], Loss: 3896.4097, Val Loss: 5563.7051\n",
      "Epoch [67/100], Loss: 3894.4297, Val Loss: 5561.3892\n",
      "Epoch [68/100], Loss: 3892.6641, Val Loss: 5559.3232\n",
      "Epoch [69/100], Loss: 3891.0918, Val Loss: 5557.4795\n",
      "Epoch [70/100], Loss: 3889.6902, Val Loss: 5555.8379\n",
      "Epoch [71/100], Loss: 3888.4434, Val Loss: 5554.3740\n",
      "Epoch [72/100], Loss: 3887.3345, Val Loss: 5553.0698\n",
      "Epoch [73/100], Loss: 3886.3464, Val Loss: 5551.9082\n",
      "Epoch [74/100], Loss: 3885.4675, Val Loss: 5550.8745\n",
      "Epoch [75/100], Loss: 3884.6870, Val Loss: 5549.9541\n",
      "Epoch [76/100], Loss: 3883.9941, Val Loss: 5549.1357\n",
      "Epoch [77/100], Loss: 3883.3784, Val Loss: 5548.4058\n",
      "Epoch [78/100], Loss: 3882.8315, Val Loss: 5547.7583\n",
      "Epoch [79/100], Loss: 3882.3462, Val Loss: 5547.1831\n",
      "Epoch [80/100], Loss: 3881.9138, Val Loss: 5546.6694\n",
      "Epoch [81/100], Loss: 3881.5312, Val Loss: 5546.2139\n",
      "Epoch [82/100], Loss: 3881.1924, Val Loss: 5545.8086\n",
      "Epoch [83/100], Loss: 3880.8914, Val Loss: 5545.4478\n",
      "Epoch [84/100], Loss: 3880.6245, Val Loss: 5545.1284\n",
      "Epoch [85/100], Loss: 3880.3887, Val Loss: 5544.8452\n",
      "Epoch [86/100], Loss: 3880.1968, Val Loss: 5544.6089\n",
      "Epoch [87/100], Loss: 3880.0085, Val Loss: 5544.3804\n",
      "Epoch [88/100], Loss: 3879.8394, Val Loss: 5544.1738\n",
      "Epoch [89/100], Loss: 3879.6909, Val Loss: 5543.9951\n",
      "Epoch [90/100], Loss: 3879.6362, Val Loss: 5543.9121\n",
      "Epoch [91/100], Loss: 3879.5781, Val Loss: 5543.8052\n",
      "Epoch [92/100], Loss: 3879.4460, Val Loss: 5543.6484\n",
      "Epoch [93/100], Loss: 3879.3564, Val Loss: 5543.5244\n",
      "Epoch [94/100], Loss: 3879.2878, Val Loss: 5543.3843\n",
      "Epoch [95/100], Loss: 3879.1804, Val Loss: 5543.2944\n",
      "Epoch [96/100], Loss: 3878.9695, Val Loss: 5542.9492\n",
      "Epoch [97/100], Loss: 3878.6267, Val Loss: 5542.6787\n",
      "Epoch [98/100], Loss: 3878.7222, Val Loss: 5542.6050\n",
      "Epoch [99/100], Loss: 4205.3892, Val Loss: 5909.3335\n",
      "Epoch [100/100], Loss: 4181.2441, Val Loss: 5892.3228\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size].view(-1, 1)  # Reshape target to match model output\n",
    "        \n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val.view(-1, 1))  # Reshape target to match model output\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TensorRT-optimized LSTM model (optional)\n",
    "# import torch_tensorrt\n",
    "\n",
    "# # Compile the model with TensorRT\n",
    "# trt_model = torch_tensorrt.compile(model, \n",
    "#     inputs=[torch_tensorrt.Input((batch_size, seq_length, input_size))],\n",
    "#     enabled_precisions={torch.float32, torch.float16} # Enable FP32 and FP16\n",
    "# )\n",
    "\n",
    "# # Save the TensorRT optimized model\n",
    "# torch.jit.save(trt_model, \"trt_lstm_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7343.1733\n",
      "RMSE: 16505.3264\n",
      "MAE: 15354.7496\n",
      "MAPE: 54.55%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test.view(-1, 1))  # Ensure target shape matches output\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# Inverse transform predictions\n",
    "# Assuming the MinMaxScaler was applied only to the target variable\n",
    "scaler = preprocessor.named_transformers_['num']  # Access the MinMaxScaler for numeric columns\n",
    "\n",
    "# Inverse transform only the target variable\n",
    "# Select the column index corresponding to the target variable\n",
    "target_index = list(numeric_columns).index(f'{symbol}_Close')\n",
    "predictions = scaler.inverse_transform(np.concatenate([np.zeros((test_outputs.shape[0], target_index)), \n",
    "                                                       test_outputs.cpu().numpy(), \n",
    "                                                       np.zeros((test_outputs.shape[0], len(numeric_columns) - target_index - 1))], axis=1))[:, target_index]\n",
    "actual = scaler.inverse_transform(np.concatenate([np.zeros((y_test.shape[0], target_index)), \n",
    "                                                  y_test.cpu().numpy().reshape(-1, 1), \n",
    "                                                  np.zeros((y_test.shape[0], len(numeric_columns) - target_index - 1))], axis=1))[:, target_index]\n",
    "\n",
    "# Calculate metrics (e.g., RMSE, MAE)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "mae = mean_absolute_error(actual, predictions)\n",
    "\n",
    "# Calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_indices = y_true != 0  # Avoid division by zero\n",
    "    return np.mean(np.abs((y_true[non_zero_indices] - y_pred[non_zero_indices]) / y_true[non_zero_indices])) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(actual, predictions)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
